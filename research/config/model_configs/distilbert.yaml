# DistilBERT Model Configuration

model:
  name: "distilbert-base-uncased"
  type: "distilbert"
  tokenizer: "distilbert-base-uncased"
  num_labels: 2

  # Model info
  parameters: "66M"
  description: "Fastest transformer, good baseline performance"

# Optional: Model-specific training overrides
# training:
#   per_device_train_batch_size: 32  # DistilBERT can handle larger batches
